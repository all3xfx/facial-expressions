{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Importing the required packages\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>pixels</th>\n",
       "      <th>Usage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   emotion                                             pixels     Usage\n",
       "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n",
       "1        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training\n",
       "2        2  231 212 156 164 174 138 161 173 182 200 106 38...  Training\n",
       "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training\n",
       "4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('fer2013.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              pixels\n",
       "0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...\n",
       "1  151 150 147 155 148 133 111 140 170 174 182 15...\n",
       "2  231 212 156 164 174 138 161 173 182 200 106 38...\n",
       "3  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...\n",
       "4  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separating the data into labels and features\n",
    "labels, data = data['emotion'], data.drop(['emotion','Usage'], axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data = data[0:34000]\n",
    "labels = labels[0:34000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fa6debb42e8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nXvjGNIgXvjEN4oVvTIN44RvTIP8GhAeK/H0M050AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa6e105c3c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Taking in the pixels of image i.e the features as a numpy array\n",
    "pixel_data = data.values\n",
    "pixel_data = pixel_data.astype(str)\n",
    "pixel_data = np.core.defchararray.rsplit(pixel_data, sep=None, maxsplit=None)\n",
    "\n",
    "# Visualizing the images using matplotlib\n",
    "first_row = np.asarray((pixel_data)[0][0]).astype('float32')\n",
    "vis_image = first_row.reshape(48,48)\n",
    "plt.imshow(vis_image,cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Reshaping Data\n",
    "\n",
    "This entire part focuses on reshaping the data to be fed into the placeholder tensors which are defined further in the code. This was probably one of the hardest parts of the code and had to be done after reading an extensive amount of numpy documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17, 2000, 48, 48, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining an empty list to hold values\n",
    "image_data = []\n",
    "# Here we parse through each element in pixel_data, extract and reshape the images and then finally resize it appropriately\n",
    "for i in range(len(pixel_data)):\n",
    "    row_data = np.asarray((pixel_data)[i][0]).astype('float32')\n",
    "    # Normalizing the data to a value between 0 and 1\n",
    "    row_data = row_data/255.0\n",
    "    reshaped_row_data = row_data.reshape(48,48)\n",
    "    image_data.append(reshaped_row_data)\n",
    "big_data = np.vstack(image_data)\n",
    "\n",
    "'''\n",
    "The shape of the array holding image data should be of size (number of batches, number of image samples, height, width, \n",
    "number of input color channels)\n",
    "'''\n",
    "final_data = big_data.reshape(34000,48,48,1)\n",
    "final_data = big_data.reshape(17,2000,48,48,1)\n",
    "final_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17, 2000, 7)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's set aside the image data and process the labels. Since labels are categorical variables we can one-hot-encode them  \n",
    "encoded_labels = pd.get_dummies(labels)\n",
    "encoded_labels = encoded_labels.as_matrix()\n",
    "encoded_labels = encoded_labels.reshape(17,2000,7)\n",
    "encoded_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Now to split the data into training and validation split\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cross_validation import ShuffleSplit\n",
    "'''\n",
    "data_train, labels_train : Data and labels for training model \n",
    "data_test, labels_test : Data and labels for testing the model \n",
    "data_valid, labels_valid : Data and labels for cross validation\n",
    "'''\n",
    "data_train, data_test, labels_train, labels_test = train_test_split(final_data, encoded_labels, test_size=0.01, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_test = data_test.reshape(2000, 48, 48, 1)\n",
    "labels_test = labels_test.reshape(2000, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def neural_net_image_input(image_shape):\n",
    "    # Returns a tensor for holding input images\n",
    "    return tf.placeholder(tf.float32,shape=[None,image_shape[0],image_shape[1],image_shape[2]], name='x')\n",
    "         \n",
    "def neural_net_label_input(n_classes):\n",
    "    # Returns a tensor for holding image labels  \n",
    "    return tf.placeholder(tf.float32,shape=[None, n_classes], name='y')\n",
    "     \n",
    "def neural_net_keep_prob_input():\n",
    "    # Return a Tensor for keep probability\n",
    "    return tf.placeholder(tf.float32, shape=None, name='keep_prob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    depth = int(x_tensor.get_shape().as_list()[3])\n",
    "    weights = tf.Variable(tf.truncated_normal([conv_ksize[0],conv_ksize[1],depth,conv_num_outputs], stddev=0.05))\n",
    "    bias = tf.Variable(tf.zeros([conv_num_outputs]))\n",
    "    \n",
    "    # Convolutional layer + Maxpooling layer\n",
    "    conv_layer = tf.nn.conv2d(x_tensor, weights, strides=[1,conv_strides[0],conv_strides[1], 1], padding='SAME')\n",
    "    conv_layer = tf.nn.bias_add(conv_layer, bias)\n",
    "    conv_layer = tf.nn.relu(conv_layer)\n",
    "    conv_layer = tf.nn.max_pool(conv_layer, ksize=[1, pool_ksize[0], pool_ksize[1],1], strides=[1, pool_strides[0], pool_strides[1], 1], padding=\"SAME\")\n",
    "    return conv_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # Storing the dimensions of the tensor as a list into a variable to make it easily accessible.\n",
    "    list_dim = x_tensor.get_shape().as_list()\n",
    "    # Here the reshape function returns a tensor of reduced dimensionality, 2-D in this case with batch size as none.\n",
    "    flattened_tensor = tf.reshape(x_tensor, [-1, list_dim[1]*list_dim[2]*list_dim[3]])\n",
    "    return flattened_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    dimension = x_tensor.get_shape().as_list()[1]\n",
    "    # Here I'm defining separate weights and bias for the fully connected layer..these weights live inside of this function.\n",
    "    weights_full = tf.Variable(tf.truncated_normal([dimension, num_outputs], stddev = 0.05))\n",
    "    bias_full = tf.Variable(tf.zeros([num_outputs]))\n",
    "    # This is a regular hidden layer with weights and biases.\n",
    "    fully_connected = tf.add(tf.matmul(x_tensor, weights_full),bias_full)\n",
    "    fully_connected = tf.nn.relu(fully_connected)\n",
    "    return fully_connected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    new_dimension = x_tensor.get_shape().as_list()[1]\n",
    "    # This is very similar to the fully connected layer.\n",
    "    weights_out = tf.Variable(tf.truncated_normal([new_dimension, num_outputs], stddev = 0.05))\n",
    "    bias_out = tf.Variable(tf.zeros([num_outputs]))\n",
    "    #Implementing the output layer.\n",
    "    output_layer = tf.add(tf.matmul(x_tensor, weights_out), bias_out)\n",
    "    return output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\" \n",
    "    # The first three convolutional + maxpooling layers\n",
    "    neural_net = conv2d_maxpool(x, 32, (5,5), (1,1), (2,2), (1,1))\n",
    "    neural_net = conv2d_maxpool(neural_net, 64, (1,1), (2,2), (4,4), (2,2))\n",
    "    neural_net = conv2d_maxpool(neural_net, 64, (2,2), (1,1), (2,2), (1,1))\n",
    "\n",
    "    # Layer to flatten the tensor from the convolutional layers\n",
    "    neural_net = flatten(neural_net)\n",
    "    \n",
    "    # Two fully connected layers with dropout\n",
    "    neural_net = fully_conn(neural_net, 2304)\n",
    "    neural_net = tf.nn.dropout(neural_net, keep_prob)\n",
    "    neural_net = fully_conn(neural_net, 2304)\n",
    "    neural_net = tf.nn.dropout(neural_net, keep_prob)\n",
    "    neural_net = fully_conn(neural_net, 2304)\n",
    "    \n",
    "    # Layer for output\n",
    "    neural_net = output(neural_net, 7)\n",
    "    \n",
    "    # TODO: return output\n",
    "    return neural_net\n",
    "\n",
    "##########Building the Neural Network##########\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((48, 48, 1))\n",
    "y = neural_net_label_input(7)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Two functions to train the network and print stats \n",
    "def train_network(session, optimizer, keep_probability, features,labels):\n",
    "    \n",
    "    # Optimizing the neural net\n",
    "    session.run(optimizer, feed_dict={x:features,y:labels, keep_prob:keep_probability})\n",
    "    pass\n",
    "\n",
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \n",
    "    # Calculating loss and validation accuracy\n",
    "    loss = sess.run(cost, feed_dict={x: feature_batch, y: label_batch, keep_prob: 1.})\n",
    "    valid_acc = sess.run(accuracy, feed_dict={\n",
    "                x: data_test,\n",
    "                y: labels_test,\n",
    "                keep_prob: 1.})\n",
    "    print(\"loss\", loss, \"valid_acc\", valid_acc)\n",
    "    pass    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Defining the hyperparameters\n",
    "epochs = 20\n",
    "batch_size = 200\n",
    "keep_probability = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on the data\n",
      "loss 1.84387 valid_acc 0.2635\n",
      "loss 1.8277 valid_acc 0.2635\n",
      "loss 1.81774 valid_acc 0.2635\n",
      "loss 1.84219 valid_acc 0.2635\n",
      "loss 1.77836 valid_acc 0.256\n",
      "loss 1.79159 valid_acc 0.2655\n",
      "loss 1.74969 valid_acc 0.2825\n",
      "loss 1.73234 valid_acc 0.287\n",
      "loss 1.69969 valid_acc 0.302\n",
      "loss 1.79891 valid_acc 0.305\n",
      "loss 1.68931 valid_acc 0.309\n",
      "loss 1.641 valid_acc 0.314\n",
      "loss 1.67679 valid_acc 0.3125\n",
      "loss 1.67697 valid_acc 0.303\n",
      "loss 1.67907 valid_acc 0.314\n",
      "loss 1.71116 valid_acc 0.331\n",
      "loss 1.62507 valid_acc 0.3185\n",
      "loss 1.61103 valid_acc 0.3275\n",
      "loss 1.64062 valid_acc 0.3495\n",
      "loss 1.64498 valid_acc 0.3415\n",
      "loss 1.58875 valid_acc 0.3535\n",
      "loss 1.64668 valid_acc 0.3715\n",
      "loss 1.54991 valid_acc 0.351\n",
      "loss 1.55641 valid_acc 0.362\n",
      "loss 1.65466 valid_acc 0.3575\n",
      "loss 1.68627 valid_acc 0.349\n",
      "loss 1.51842 valid_acc 0.372\n",
      "loss 1.4717 valid_acc 0.4025\n",
      "loss 1.44941 valid_acc 0.3965\n",
      "loss 1.55226 valid_acc 0.3875\n",
      "loss 1.50197 valid_acc 0.403\n",
      "loss 1.49166 valid_acc 0.4\n",
      "loss 1.50846 valid_acc 0.4035\n",
      "loss 1.46809 valid_acc 0.4105\n",
      "loss 1.4813 valid_acc 0.416\n",
      "loss 1.52227 valid_acc 0.3875\n",
      "loss 1.44158 valid_acc 0.4025\n",
      "loss 1.51244 valid_acc 0.4155\n",
      "loss 1.39863 valid_acc 0.415\n",
      "loss 1.47633 valid_acc 0.414\n",
      "loss 1.47759 valid_acc 0.4265\n",
      "loss 1.53707 valid_acc 0.4015\n",
      "loss 1.38025 valid_acc 0.444\n",
      "loss 1.35739 valid_acc 0.4405\n",
      "loss 1.31404 valid_acc 0.442\n",
      "loss 1.41406 valid_acc 0.431\n",
      "loss 1.42175 valid_acc 0.431\n",
      "loss 1.42095 valid_acc 0.411\n",
      "loss 1.41665 valid_acc 0.434\n",
      "loss 1.43101 valid_acc 0.434\n",
      "loss 1.40038 valid_acc 0.4435\n",
      "loss 1.40671 valid_acc 0.4415\n",
      "loss 1.31984 valid_acc 0.4565\n",
      "loss 1.39828 valid_acc 0.4635\n",
      "loss 1.29197 valid_acc 0.4585\n",
      "loss 1.42319 valid_acc 0.4385\n",
      "loss 1.39747 valid_acc 0.4545\n",
      "loss 1.45556 valid_acc 0.438\n",
      "loss 1.28914 valid_acc 0.4565\n",
      "loss 1.30323 valid_acc 0.462\n",
      "loss 1.24271 valid_acc 0.475\n",
      "loss 1.39827 valid_acc 0.4445\n",
      "loss 1.39286 valid_acc 0.4355\n",
      "loss 1.30229 valid_acc 0.4525\n",
      "loss 1.37088 valid_acc 0.463\n",
      "loss 1.34503 valid_acc 0.4635\n",
      "loss 1.30386 valid_acc 0.477\n",
      "loss 1.35841 valid_acc 0.4525\n",
      "loss 1.26362 valid_acc 0.4765\n",
      "loss 1.31585 valid_acc 0.4795\n",
      "loss 1.24615 valid_acc 0.48\n",
      "loss 1.36344 valid_acc 0.448\n",
      "loss 1.33449 valid_acc 0.4755\n",
      "loss 1.38853 valid_acc 0.456\n",
      "loss 1.23234 valid_acc 0.4685\n",
      "loss 1.25149 valid_acc 0.463\n",
      "loss 1.26298 valid_acc 0.467\n",
      "loss 1.32069 valid_acc 0.455\n",
      "loss 1.32594 valid_acc 0.4565\n",
      "loss 1.30546 valid_acc 0.4595\n",
      "loss 1.31754 valid_acc 0.474\n",
      "loss 1.32382 valid_acc 0.4715\n",
      "loss 1.25186 valid_acc 0.473\n",
      "loss 1.28568 valid_acc 0.485\n",
      "loss 1.21663 valid_acc 0.4805\n",
      "loss 1.24402 valid_acc 0.481\n",
      "loss 1.23866 valid_acc 0.4755\n",
      "loss 1.2583 valid_acc 0.484\n",
      "loss 1.2795 valid_acc 0.4825\n",
      "loss 1.32353 valid_acc 0.479\n",
      "loss 1.18189 valid_acc 0.491\n",
      "loss 1.20139 valid_acc 0.4945\n",
      "loss 1.18973 valid_acc 0.48\n",
      "loss 1.26712 valid_acc 0.4915\n",
      "loss 1.28052 valid_acc 0.47\n",
      "loss 1.25267 valid_acc 0.476\n",
      "loss 1.2782 valid_acc 0.472\n",
      "loss 1.29635 valid_acc 0.4735\n",
      "loss 1.22435 valid_acc 0.4985\n",
      "loss 1.24238 valid_acc 0.486\n",
      "loss 1.1316 valid_acc 0.491\n",
      "loss 1.25103 valid_acc 0.4865\n",
      "loss 1.19853 valid_acc 0.481\n",
      "loss 1.22668 valid_acc 0.5035\n",
      "loss 1.2464 valid_acc 0.4985\n",
      "loss 1.32812 valid_acc 0.4835\n",
      "loss 1.15047 valid_acc 0.4905\n",
      "loss 1.14493 valid_acc 0.4925\n",
      "loss 1.17262 valid_acc 0.4905\n",
      "loss 1.2503 valid_acc 0.498\n",
      "loss 1.28491 valid_acc 0.4685\n",
      "loss 1.17678 valid_acc 0.476\n",
      "loss 1.26666 valid_acc 0.495\n",
      "loss 1.24821 valid_acc 0.4775\n",
      "loss 1.1806 valid_acc 0.492\n",
      "loss 1.24599 valid_acc 0.483\n",
      "loss 1.09316 valid_acc 0.4955\n",
      "loss 1.19914 valid_acc 0.492\n",
      "loss 1.14649 valid_acc 0.4935\n",
      "loss 1.22102 valid_acc 0.498\n",
      "loss 1.19906 valid_acc 0.4975\n",
      "loss 1.20078 valid_acc 0.4875\n",
      "loss 1.10602 valid_acc 0.4955\n",
      "loss 1.12774 valid_acc 0.502\n",
      "loss 1.15916 valid_acc 0.502\n",
      "loss 1.1935 valid_acc 0.5\n",
      "loss 1.22353 valid_acc 0.4935\n",
      "loss 1.12602 valid_acc 0.49\n",
      "loss 1.2079 valid_acc 0.5115\n",
      "loss 1.19753 valid_acc 0.485\n",
      "loss 1.14789 valid_acc 0.5165\n",
      "loss 1.19714 valid_acc 0.509\n",
      "loss 1.06876 valid_acc 0.5045\n",
      "loss 1.14921 valid_acc 0.505\n",
      "loss 1.11052 valid_acc 0.4955\n",
      "loss 1.17131 valid_acc 0.52\n",
      "loss 1.18511 valid_acc 0.5065\n",
      "loss 1.18818 valid_acc 0.4985\n",
      "loss 1.05119 valid_acc 0.508\n",
      "loss 1.10271 valid_acc 0.5165\n",
      "loss 1.11569 valid_acc 0.508\n",
      "loss 1.14992 valid_acc 0.501\n",
      "loss 1.18201 valid_acc 0.5015\n",
      "loss 1.08248 valid_acc 0.4965\n",
      "loss 1.19111 valid_acc 0.4935\n",
      "loss 1.12468 valid_acc 0.4805\n",
      "loss 1.05724 valid_acc 0.501\n",
      "loss 1.16914 valid_acc 0.514\n",
      "loss 1.02779 valid_acc 0.5\n",
      "loss 1.08907 valid_acc 0.511\n",
      "loss 1.06068 valid_acc 0.4955\n",
      "loss 1.1291 valid_acc 0.502\n",
      "loss 1.12736 valid_acc 0.4985\n",
      "loss 1.16291 valid_acc 0.4855\n",
      "loss 1.0537 valid_acc 0.499\n",
      "loss 1.04698 valid_acc 0.5175\n",
      "loss 1.10685 valid_acc 0.504\n",
      "loss 1.09474 valid_acc 0.508\n",
      "loss 1.13767 valid_acc 0.5075\n",
      "loss 1.06259 valid_acc 0.506\n",
      "loss 1.14511 valid_acc 0.5095\n",
      "loss 1.09838 valid_acc 0.498\n",
      "loss 1.08039 valid_acc 0.5085\n",
      "loss 1.10751 valid_acc 0.5085\n",
      "loss 0.973927 valid_acc 0.5065\n",
      "loss 1.09999 valid_acc 0.505\n",
      "loss 1.00439 valid_acc 0.5085\n",
      "loss 1.11618 valid_acc 0.517\n",
      "loss 1.10823 valid_acc 0.5105\n",
      "loss 1.11728 valid_acc 0.4955\n",
      "loss 1.01713 valid_acc 0.497\n",
      "loss 0.999271 valid_acc 0.5305\n",
      "loss 1.03672 valid_acc 0.514\n",
      "loss 1.09558 valid_acc 0.5135\n",
      "loss 1.14551 valid_acc 0.5025\n",
      "loss 1.0028 valid_acc 0.508\n",
      "loss 1.09184 valid_acc 0.514\n",
      "loss 1.03474 valid_acc 0.515\n",
      "loss 0.990153 valid_acc 0.516\n",
      "loss 1.05077 valid_acc 0.517\n",
      "loss 0.96125 valid_acc 0.502\n",
      "loss 1.06409 valid_acc 0.5195\n",
      "loss 0.963981 valid_acc 0.5225\n",
      "loss 1.04825 valid_acc 0.522\n",
      "loss 1.07013 valid_acc 0.5005\n",
      "loss 1.09925 valid_acc 0.496\n",
      "loss 0.973124 valid_acc 0.5235\n",
      "loss 0.992322 valid_acc 0.5155\n",
      "loss 0.976787 valid_acc 0.5325\n",
      "loss 1.05011 valid_acc 0.5275\n",
      "loss 1.12014 valid_acc 0.5085\n",
      "loss 0.957361 valid_acc 0.516\n",
      "loss 1.07582 valid_acc 0.502\n",
      "loss 1.01642 valid_acc 0.512\n",
      "loss 0.948881 valid_acc 0.5175\n",
      "loss 1.05062 valid_acc 0.518\n",
      "loss 0.946069 valid_acc 0.5085\n",
      "loss 1.00183 valid_acc 0.529\n",
      "loss 0.898267 valid_acc 0.5205\n",
      "loss 1.05884 valid_acc 0.52\n",
      "loss 1.0184 valid_acc 0.512\n",
      "loss 1.02323 valid_acc 0.506\n",
      "loss 0.935229 valid_acc 0.524\n",
      "loss 0.932417 valid_acc 0.5195\n",
      "loss 0.941432 valid_acc 0.527\n",
      "loss 1.00443 valid_acc 0.5185\n",
      "loss 1.07912 valid_acc 0.5045\n",
      "loss 0.958072 valid_acc 0.512\n",
      "loss 1.06851 valid_acc 0.5235\n",
      "loss 1.03604 valid_acc 0.513\n",
      "loss 0.901452 valid_acc 0.5165\n",
      "loss 1.00947 valid_acc 0.523\n",
      "loss 0.936028 valid_acc 0.4965\n",
      "loss 0.965465 valid_acc 0.521\n",
      "loss 0.874347 valid_acc 0.5055\n",
      "loss 0.961652 valid_acc 0.5255\n",
      "loss 0.981268 valid_acc 0.5045\n",
      "loss 0.986258 valid_acc 0.5125\n",
      "loss 0.897728 valid_acc 0.527\n",
      "loss 0.903235 valid_acc 0.53\n",
      "loss 0.910715 valid_acc 0.5285\n",
      "loss 0.957451 valid_acc 0.5155\n",
      "loss 1.06374 valid_acc 0.513\n",
      "loss 0.941536 valid_acc 0.512\n",
      "loss 1.00803 valid_acc 0.5375\n",
      "loss 0.952676 valid_acc 0.5215\n",
      "loss 0.836751 valid_acc 0.513\n",
      "loss 0.968921 valid_acc 0.522\n",
      "loss 0.880729 valid_acc 0.513\n",
      "loss 0.946126 valid_acc 0.52\n",
      "loss 0.857806 valid_acc 0.5265\n",
      "loss 0.919638 valid_acc 0.5125\n",
      "loss 0.974503 valid_acc 0.5145\n",
      "loss 0.942846 valid_acc 0.5095\n",
      "loss 0.895829 valid_acc 0.5185\n",
      "loss 0.857576 valid_acc 0.526\n",
      "loss 0.881871 valid_acc 0.515\n",
      "loss 0.939286 valid_acc 0.525\n",
      "loss 1.03251 valid_acc 0.5235\n",
      "loss 0.89126 valid_acc 0.5305\n",
      "loss 1.00655 valid_acc 0.5315\n",
      "loss 0.872271 valid_acc 0.519\n",
      "loss 0.765856 valid_acc 0.5185\n",
      "loss 0.975814 valid_acc 0.525\n",
      "loss 0.883616 valid_acc 0.5065\n",
      "loss 0.982519 valid_acc 0.513\n",
      "loss 0.838511 valid_acc 0.52\n",
      "loss 0.905737 valid_acc 0.5245\n",
      "loss 0.923607 valid_acc 0.524\n",
      "loss 0.831381 valid_acc 0.5175\n",
      "loss 0.834908 valid_acc 0.516\n",
      "loss 0.860249 valid_acc 0.512\n",
      "loss 0.88488 valid_acc 0.518\n",
      "loss 0.92015 valid_acc 0.5215\n",
      "loss 0.993699 valid_acc 0.51\n",
      "loss 0.911397 valid_acc 0.526\n",
      "loss 0.983634 valid_acc 0.522\n",
      "loss 0.867459 valid_acc 0.5255\n",
      "loss 0.752088 valid_acc 0.501\n",
      "loss 0.971328 valid_acc 0.5305\n",
      "loss 0.8587 valid_acc 0.5155\n",
      "loss 0.935692 valid_acc 0.517\n",
      "loss 0.829751 valid_acc 0.511\n",
      "loss 0.862309 valid_acc 0.5185\n",
      "loss 0.89508 valid_acc 0.5315\n",
      "loss 0.809972 valid_acc 0.5225\n",
      "loss 0.797482 valid_acc 0.513\n",
      "loss 0.809884 valid_acc 0.523\n",
      "loss 0.827493 valid_acc 0.5225\n",
      "loss 0.882876 valid_acc 0.528\n",
      "loss 0.934188 valid_acc 0.5095\n",
      "loss 0.869339 valid_acc 0.523\n",
      "loss 0.967749 valid_acc 0.53\n",
      "loss 0.763809 valid_acc 0.5315\n",
      "loss 0.689385 valid_acc 0.513\n",
      "loss 1.0495 valid_acc 0.5105\n",
      "loss 0.822792 valid_acc 0.5185\n",
      "loss 0.908544 valid_acc 0.5325\n",
      "loss 0.797411 valid_acc 0.517\n",
      "loss 0.816377 valid_acc 0.5155\n",
      "loss 0.841016 valid_acc 0.5295\n",
      "loss 0.780015 valid_acc 0.5225\n",
      "loss 0.770241 valid_acc 0.518\n",
      "loss 0.787738 valid_acc 0.5265\n",
      "loss 0.808104 valid_acc 0.526\n",
      "loss 0.841677 valid_acc 0.5155\n",
      "loss 0.896884 valid_acc 0.525\n",
      "loss 0.792284 valid_acc 0.529\n",
      "loss 0.883597 valid_acc 0.5295\n",
      "loss 0.746643 valid_acc 0.508\n",
      "loss 0.613331 valid_acc 0.523\n",
      "loss 0.921907 valid_acc 0.531\n",
      "loss 0.796473 valid_acc 0.529\n",
      "loss 0.855162 valid_acc 0.5395\n",
      "loss 0.761164 valid_acc 0.5285\n",
      "loss 0.700516 valid_acc 0.511\n",
      "loss 0.788704 valid_acc 0.531\n",
      "loss 0.752561 valid_acc 0.515\n",
      "loss 0.772038 valid_acc 0.5185\n",
      "loss 0.705979 valid_acc 0.528\n",
      "loss 0.741567 valid_acc 0.531\n",
      "loss 0.760816 valid_acc 0.53\n",
      "loss 0.837644 valid_acc 0.523\n",
      "loss 0.74795 valid_acc 0.5315\n",
      "loss 0.875068 valid_acc 0.5365\n",
      "loss 0.642562 valid_acc 0.544\n",
      "loss 0.586935 valid_acc 0.509\n",
      "loss 0.821074 valid_acc 0.5185\n",
      "loss 0.754861 valid_acc 0.5335\n",
      "loss 0.859319 valid_acc 0.5315\n",
      "loss 0.737908 valid_acc 0.515\n",
      "loss 0.661802 valid_acc 0.514\n",
      "loss 0.73751 valid_acc 0.5195\n",
      "loss 0.675937 valid_acc 0.5065\n",
      "loss 0.675366 valid_acc 0.529\n",
      "loss 0.707528 valid_acc 0.516\n",
      "loss 0.70082 valid_acc 0.537\n",
      "loss 0.725381 valid_acc 0.519\n",
      "loss 0.79879 valid_acc 0.516\n",
      "loss 0.707978 valid_acc 0.5385\n"
     ]
    }
   ],
   "source": [
    "# Finally let's get training \n",
    "print(\"Training on the data\")\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        num_batches = 16\n",
    "        for batch in range(num_batches):\n",
    "            for offset in range(0, 2000, batch_size):\n",
    "                batch_x, batch_y = data_train[batch][offset:offset+batch_size], labels_train[batch][offset:offset+batch_size]\n",
    "                train_network(sess, optimizer, keep_probability, batch_x, batch_y )\n",
    "            print_stats(sess, batch_x, batch_y, cost, accuracy)\n",
    "       "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
