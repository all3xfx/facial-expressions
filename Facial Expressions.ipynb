{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Importing the required packages\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>pixels</th>\n",
       "      <th>Usage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   emotion                                             pixels     Usage\n",
       "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n",
       "1        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training\n",
       "2        2  231 212 156 164 174 138 161 173 182 200 106 38...  Training\n",
       "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training\n",
       "4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('fer2013.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              pixels\n",
       "0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...\n",
       "1  151 150 147 155 148 133 111 140 170 174 182 15...\n",
       "2  231 212 156 164 174 138 161 173 182 200 106 38...\n",
       "3  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...\n",
       "4  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separating the data into labels and features\n",
    "labels, data = data['emotion'], data.drop(['emotion','Usage'], axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data = data[0:34000]\n",
    "labels = labels[0:34000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f4b262d1f98>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nXvjGNIgXvjEN4oVvTIN44RvTIP8GhAeK/H0M050AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4b2869f1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Taking in the pixels of image i.e the features as a numpy array\n",
    "pixel_data = data.values\n",
    "pixel_data = pixel_data.astype(str)\n",
    "pixel_data = np.core.defchararray.rsplit(pixel_data, sep=None, maxsplit=None)\n",
    "\n",
    "# Visualizing the images using matplotlib\n",
    "first_row = np.asarray((pixel_data)[0][0]).astype('float32')\n",
    "vis_image = first_row.reshape(48,48)\n",
    "plt.imshow(vis_image,cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Reshaping Data\n",
    "\n",
    "This entire part focuses on reshaping the data to be fed into the placeholder tensors which are defined further in the code. This was probably one of the hardest parts of the code and had to be done after reading an extensive amount of numpy documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17, 2000, 48, 48, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining an empty list to hold values\n",
    "image_data = []\n",
    "# Here we parse through each element in pixel_data, extract and reshape the images and then finally resize it appropriately\n",
    "for i in range(len(pixel_data)):\n",
    "    row_data = np.asarray((pixel_data)[i][0]).astype('float32')\n",
    "    # Normalizing the data to a value between 0 and 1\n",
    "    row_data = row_data/255.0\n",
    "    reshaped_row_data = row_data.reshape(48,48)\n",
    "    image_data.append(reshaped_row_data)\n",
    "big_data = np.vstack(image_data)\n",
    "\n",
    "'''\n",
    "The shape of the array holding image data should be of size (number of batches, number of image samples, height, width, \n",
    "number of input color channels)\n",
    "'''\n",
    "final_data = big_data.reshape(34000,48,48,1)\n",
    "final_data = big_data.reshape(17,2000,48,48,1)\n",
    "final_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17, 2000, 7)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's set aside the image data and process the labels. Since labels are categorical variables we can one-hot-encode them  \n",
    "encoded_labels = pd.get_dummies(labels)\n",
    "encoded_labels = encoded_labels.as_matrix()\n",
    "encoded_labels = encoded_labels.reshape(17,2000,7)\n",
    "encoded_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Now to split the data into training and validation split\n",
    "from sklearn.cross_validation import train_test_split\n",
    "'''\n",
    "data_train, labels_train : Data and labels for training model \n",
    "data_test, labels_test : Data and labels for cross_validation \n",
    "'''\n",
    "data_train, data_test, labels_train, labels_test = train_test_split(final_data, encoded_labels, test_size=0.01, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_test = data_test.reshape(2000, 48, 48, 1)\n",
    "labels_test = labels_test.reshape(2000, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def neural_net_image_input(image_shape):\n",
    "    # Returns a tensor for holding input images\n",
    "    return tf.placeholder(tf.float32,shape=[None,image_shape[0],image_shape[1],image_shape[2]], name='x')\n",
    "         \n",
    "def neural_net_label_input(n_classes):\n",
    "    # Returns a tensor for holding image labels  \n",
    "    return tf.placeholder(tf.float32,shape=[None, n_classes], name='y')\n",
    "     \n",
    "def neural_net_keep_prob_input():\n",
    "    # Return a Tensor for keep probability\n",
    "    return tf.placeholder(tf.float32, shape=None, name='keep_prob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    depth = int(x_tensor.get_shape().as_list()[3])\n",
    "    weights = tf.Variable(tf.truncated_normal([conv_ksize[0],conv_ksize[1],depth,conv_num_outputs], stddev=0.05))\n",
    "    bias = tf.Variable(tf.zeros([conv_num_outputs]))\n",
    "    \n",
    "    # Convolutional layer + Maxpooling layer\n",
    "    conv_layer = tf.nn.conv2d(x_tensor, weights, strides=[1,conv_strides[0],conv_strides[1], 1], padding='SAME')\n",
    "    conv_layer = tf.nn.bias_add(conv_layer, bias)\n",
    "    conv_layer = tf.nn.relu(conv_layer)\n",
    "    conv_layer = tf.nn.max_pool(conv_layer, ksize=[1, pool_ksize[0], pool_ksize[1],1], strides=[1, pool_strides[0], pool_strides[1], 1], padding=\"SAME\")\n",
    "    return conv_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # Storing the dimensions of the tensor as a list into a variable to make it easily accessible.\n",
    "    list_dim = x_tensor.get_shape().as_list()\n",
    "    # Here the reshape function returns a tensor of reduced dimensionality, 2-D in this case with batch size as none.\n",
    "    flattened_tensor = tf.reshape(x_tensor, [-1, list_dim[1]*list_dim[2]*list_dim[3]])\n",
    "    return flattened_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    dimension = x_tensor.get_shape().as_list()[1]\n",
    "    # Here I'm defining separate weights and bias for the fully connected layer..these weights live inside of this function.\n",
    "    weights_full = tf.Variable(tf.truncated_normal([dimension, num_outputs], stddev = 0.05))\n",
    "    bias_full = tf.Variable(tf.zeros([num_outputs]))\n",
    "    # This is a regular hidden layer with weights and biases.\n",
    "    fully_connected = tf.add(tf.matmul(x_tensor, weights_full),bias_full)\n",
    "    fully_connected = tf.nn.relu(fully_connected)\n",
    "    return fully_connected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    new_dimension = x_tensor.get_shape().as_list()[1]\n",
    "    # This is very similar to the fully connected layer.\n",
    "    weights_out = tf.Variable(tf.truncated_normal([new_dimension, num_outputs], stddev = 0.05))\n",
    "    bias_out = tf.Variable(tf.zeros([num_outputs]))\n",
    "    #Implementing the output layer.\n",
    "    output_layer = tf.add(tf.matmul(x_tensor, weights_out), bias_out)\n",
    "    return output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\" \n",
    "    # The first three convolutional + maxpooling layers\n",
    "    neural_net = conv2d_maxpool(x, 32, (5,5), (1,1), (2,2), (1,1))\n",
    "    neural_net = conv2d_maxpool(neural_net, 64, (1,1), (2,2), (4,4), (2,2))\n",
    "    neural_net = conv2d_maxpool(neural_net, 64, (2,2), (1,1), (2,2), (1,1))\n",
    "    neural_net = conv2d_maxpool(neural_net, 128, (2,2), (1,1), (2,2), (1,1))\n",
    "\n",
    "    # Layer to flatten the tensor from the convolutional layers\n",
    "    neural_net = flatten(neural_net)\n",
    "    \n",
    "    # Two fully connected layers with dropout\n",
    "    neural_net = fully_conn(neural_net, 2304)\n",
    "    neural_net = tf.nn.dropout(neural_net, keep_prob)\n",
    "    neural_net = fully_conn(neural_net, 1152)\n",
    "    neural_net = tf.nn.dropout(neural_net, keep_prob)\n",
    "    neural_net = fully_conn(neural_net, 576)\n",
    "    \n",
    "    # Layer for output\n",
    "    neural_net = output(neural_net, 7)\n",
    "    \n",
    "    # TODO: return output\n",
    "    return neural_net\n",
    "\n",
    "##########Building the Neural Network##########\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((48, 48, 1))\n",
    "y = neural_net_label_input(7)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(1e-4).minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Two functions to train the network and print stats \n",
    "def train_network(session, optimizer, keep_probability, features,labels):\n",
    "    \n",
    "    # Optimizing the neural net\n",
    "    session.run(optimizer, feed_dict={x:features,y:labels, keep_prob:keep_probability})\n",
    "    pass\n",
    "\n",
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \n",
    "    # Calculating loss and validation accuracy\n",
    "    loss = sess.run(cost, feed_dict={x: feature_batch, y: label_batch, keep_prob: 1.})\n",
    "    valid_acc = sess.run(accuracy, feed_dict={\n",
    "                x: data_test,\n",
    "                y: labels_test,\n",
    "                keep_prob: 1.})\n",
    "    print(\"loss\", loss, \"valid_acc\", valid_acc)\n",
    "    pass    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Defining the hyperparameters\n",
    "epochs = 20\n",
    "batch_size = 200\n",
    "keep_probability = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on the data\n",
      "Epoch1\n",
      "Batch1\n",
      "loss 1.77382 valid_acc 0.2635\n",
      "Batch2\n",
      "loss 1.81935 valid_acc 0.2635\n",
      "Batch3\n",
      "loss 1.84463 valid_acc 0.2635\n",
      "Batch4\n",
      "loss 1.85445 valid_acc 0.2635\n",
      "Batch5\n",
      "loss 1.80688 valid_acc 0.2635\n",
      "Batch6\n",
      "loss 1.8099 valid_acc 0.2635\n",
      "Batch7\n",
      "loss 1.81062 valid_acc 0.2655\n",
      "Batch8\n",
      "loss 1.80842 valid_acc 0.2635\n",
      "Batch9\n",
      "loss 1.80376 valid_acc 0.2635\n",
      "Batch10\n",
      "loss 1.82119 valid_acc 0.2635\n",
      "Batch11\n",
      "loss 1.78445 valid_acc 0.2635\n",
      "Batch12\n",
      "loss 1.76852 valid_acc 0.265\n",
      "Batch13\n",
      "loss 1.77817 valid_acc 0.2635\n",
      "Batch14\n",
      "loss 1.76914 valid_acc 0.27\n",
      "Batch15\n",
      "loss 1.7535 valid_acc 0.267\n",
      "Batch16\n",
      "loss 1.7868 valid_acc 0.286\n",
      "Epoch2\n",
      "Batch1\n",
      "loss 1.71664 valid_acc 0.2925\n",
      "Batch2\n",
      "loss 1.76214 valid_acc 0.287\n",
      "Batch3\n",
      "loss 1.75911 valid_acc 0.2855\n",
      "Batch4\n",
      "loss 1.78143 valid_acc 0.297\n",
      "Batch5\n",
      "loss 1.72984 valid_acc 0.3075\n",
      "Batch6\n",
      "loss 1.74658 valid_acc 0.2955\n",
      "Batch7\n",
      "loss 1.70052 valid_acc 0.3295\n",
      "Batch8\n",
      "loss 1.66589 valid_acc 0.3145\n",
      "Batch9\n",
      "loss 1.6887 valid_acc 0.3075\n",
      "Batch10\n",
      "loss 1.73742 valid_acc 0.3325\n",
      "Batch11\n",
      "loss 1.6473 valid_acc 0.3465\n",
      "Batch12\n",
      "loss 1.61353 valid_acc 0.3435\n",
      "Batch13\n",
      "loss 1.64938 valid_acc 0.3605\n",
      "Batch14\n",
      "loss 1.65238 valid_acc 0.3655\n",
      "Batch15\n",
      "loss 1.62004 valid_acc 0.3615\n",
      "Batch16\n",
      "loss 1.63193 valid_acc 0.3605\n",
      "Epoch3\n",
      "Batch1\n",
      "loss 1.5708 valid_acc 0.388\n",
      "Batch2\n",
      "loss 1.58706 valid_acc 0.373\n",
      "Batch3\n",
      "loss 1.57256 valid_acc 0.377\n",
      "Batch4\n",
      "loss 1.62043 valid_acc 0.3615\n",
      "Batch5\n",
      "loss 1.54327 valid_acc 0.384\n",
      "Batch6\n",
      "loss 1.64246 valid_acc 0.391\n",
      "Batch7\n",
      "loss 1.54934 valid_acc 0.381\n",
      "Batch8\n",
      "loss 1.54335 valid_acc 0.394\n",
      "Batch9\n",
      "loss 1.62754 valid_acc 0.3985\n",
      "Batch10\n",
      "loss 1.6472 valid_acc 0.3955\n",
      "Batch11\n",
      "loss 1.51268 valid_acc 0.391\n",
      "Batch12\n",
      "loss 1.49021 valid_acc 0.403\n",
      "Batch13\n",
      "loss 1.56808 valid_acc 0.399\n",
      "Batch14\n",
      "loss 1.56354 valid_acc 0.3955\n",
      "Batch15\n",
      "loss 1.54636 valid_acc 0.413\n",
      "Batch16\n",
      "loss 1.52097 valid_acc 0.415\n",
      "Epoch4\n",
      "Batch1\n",
      "loss 1.49998 valid_acc 0.404\n",
      "Batch2\n",
      "loss 1.48831 valid_acc 0.404\n",
      "Batch3\n",
      "loss 1.50623 valid_acc 0.416\n",
      "Batch4\n",
      "loss 1.57198 valid_acc 0.3875\n",
      "Batch5\n",
      "loss 1.48488 valid_acc 0.4025\n",
      "Batch6\n",
      "loss 1.52658 valid_acc 0.4155\n",
      "Batch7\n",
      "loss 1.47502 valid_acc 0.4115\n",
      "Batch8\n",
      "loss 1.4828 valid_acc 0.4225\n",
      "Batch9\n",
      "loss 1.54049 valid_acc 0.423\n",
      "Batch10\n",
      "loss 1.57748 valid_acc 0.424\n",
      "Batch11\n",
      "loss 1.4547 valid_acc 0.416\n",
      "Batch12\n",
      "loss 1.42921 valid_acc 0.4375\n",
      "Batch13\n",
      "loss 1.49356 valid_acc 0.418\n",
      "Batch14\n",
      "loss 1.50494 valid_acc 0.417\n",
      "Batch15\n",
      "loss 1.48613 valid_acc 0.4385\n",
      "Batch16\n",
      "loss 1.44914 valid_acc 0.4335\n",
      "Epoch5\n",
      "Batch1\n",
      "loss 1.44798 valid_acc 0.4375\n",
      "Batch2\n",
      "loss 1.41649 valid_acc 0.439\n",
      "Batch3\n",
      "loss 1.45286 valid_acc 0.4435\n",
      "Batch4\n",
      "loss 1.52446 valid_acc 0.422\n",
      "Batch5\n",
      "loss 1.38906 valid_acc 0.434\n",
      "Batch6\n",
      "loss 1.45653 valid_acc 0.432\n",
      "Batch7\n",
      "loss 1.40364 valid_acc 0.4425\n",
      "Batch8\n",
      "loss 1.431 valid_acc 0.4385\n",
      "Batch9\n",
      "loss 1.47009 valid_acc 0.431\n",
      "Batch10\n",
      "loss 1.53424 valid_acc 0.4365\n",
      "Batch11\n",
      "loss 1.40494 valid_acc 0.425\n",
      "Batch12\n",
      "loss 1.40305 valid_acc 0.449\n",
      "Batch13\n",
      "loss 1.41023 valid_acc 0.4495\n",
      "Batch14\n",
      "loss 1.4665 valid_acc 0.438\n",
      "Batch15\n",
      "loss 1.43971 valid_acc 0.447\n",
      "Batch16\n",
      "loss 1.37564 valid_acc 0.4435\n",
      "Epoch6\n",
      "Batch1\n",
      "loss 1.40484 valid_acc 0.458\n",
      "Batch2\n",
      "loss 1.36165 valid_acc 0.453\n",
      "Batch3\n",
      "loss 1.40281 valid_acc 0.4505\n",
      "Batch4\n",
      "loss 1.46662 valid_acc 0.454\n",
      "Batch5\n",
      "loss 1.34689 valid_acc 0.439\n",
      "Batch6\n",
      "loss 1.40089 valid_acc 0.4565\n",
      "Batch7\n",
      "loss 1.34978 valid_acc 0.4455\n",
      "Batch8\n",
      "loss 1.36469 valid_acc 0.447\n",
      "Batch9\n",
      "loss 1.41084 valid_acc 0.452\n",
      "Batch10\n",
      "loss 1.48791 valid_acc 0.45\n",
      "Batch11\n",
      "loss 1.34727 valid_acc 0.448\n",
      "Batch12\n",
      "loss 1.36992 valid_acc 0.464\n",
      "Batch13\n",
      "loss 1.34661 valid_acc 0.4595\n",
      "Batch14\n",
      "loss 1.41369 valid_acc 0.451\n",
      "Batch15\n",
      "loss 1.39436 valid_acc 0.468\n",
      "Batch16\n",
      "loss 1.31653 valid_acc 0.4575\n",
      "Epoch7\n",
      "Batch1\n",
      "loss 1.35916 valid_acc 0.4745\n",
      "Batch2\n",
      "loss 1.3151 valid_acc 0.4695\n",
      "Batch3\n",
      "loss 1.35207 valid_acc 0.4725\n",
      "Batch4\n",
      "loss 1.41911 valid_acc 0.4665\n",
      "Batch5\n",
      "loss 1.29152 valid_acc 0.4585\n",
      "Batch6\n",
      "loss 1.34648 valid_acc 0.4775\n",
      "Batch7\n",
      "loss 1.29324 valid_acc 0.469\n",
      "Batch8\n",
      "loss 1.31697 valid_acc 0.476\n",
      "Batch9\n",
      "loss 1.34975 valid_acc 0.4665\n",
      "Batch10\n",
      "loss 1.44131 valid_acc 0.472\n",
      "Batch11\n",
      "loss 1.29311 valid_acc 0.4595\n",
      "Batch12\n",
      "loss 1.34401 valid_acc 0.468\n",
      "Batch13\n",
      "loss 1.29266 valid_acc 0.4745\n",
      "Batch14\n",
      "loss 1.37606 valid_acc 0.4675\n",
      "Batch15\n",
      "loss 1.35999 valid_acc 0.488\n",
      "Batch16\n",
      "loss 1.25773 valid_acc 0.4755\n",
      "Epoch8\n",
      "Batch1\n",
      "loss 1.3154 valid_acc 0.493\n",
      "Batch2\n",
      "loss 1.27235 valid_acc 0.482\n",
      "Batch3\n",
      "loss 1.30361 valid_acc 0.4825\n",
      "Batch4\n",
      "loss 1.33666 valid_acc 0.4875\n",
      "Batch5\n",
      "loss 1.22434 valid_acc 0.48\n",
      "Batch6\n",
      "loss 1.30319 valid_acc 0.493\n",
      "Batch7\n",
      "loss 1.25393 valid_acc 0.4805\n",
      "Batch8\n",
      "loss 1.27818 valid_acc 0.4875\n",
      "Batch9\n",
      "loss 1.30615 valid_acc 0.4895\n",
      "Batch10\n",
      "loss 1.39227 valid_acc 0.4865\n",
      "Batch11\n",
      "loss 1.2623 valid_acc 0.468\n",
      "Batch12\n",
      "loss 1.30667 valid_acc 0.489\n",
      "Batch13\n",
      "loss 1.24394 valid_acc 0.485\n",
      "Batch14\n",
      "loss 1.33545 valid_acc 0.4855\n",
      "Batch15\n",
      "loss 1.31785 valid_acc 0.4975\n",
      "Batch16\n",
      "loss 1.22762 valid_acc 0.4845\n",
      "Epoch9\n",
      "Batch1\n",
      "loss 1.29056 valid_acc 0.498\n",
      "Batch2\n",
      "loss 1.24266 valid_acc 0.4925\n",
      "Batch3\n",
      "loss 1.25461 valid_acc 0.4825\n",
      "Batch4\n",
      "loss 1.29231 valid_acc 0.498\n",
      "Batch5\n",
      "loss 1.19224 valid_acc 0.4915\n",
      "Batch6\n",
      "loss 1.25579 valid_acc 0.4975\n",
      "Batch7\n",
      "loss 1.20932 valid_acc 0.499\n",
      "Batch8\n",
      "loss 1.24558 valid_acc 0.4945\n",
      "Batch9\n",
      "loss 1.26652 valid_acc 0.4955\n",
      "Batch10\n",
      "loss 1.34891 valid_acc 0.5\n",
      "Batch11\n",
      "loss 1.20223 valid_acc 0.496\n",
      "Batch12\n",
      "loss 1.27255 valid_acc 0.4955\n",
      "Batch13\n",
      "loss 1.20457 valid_acc 0.505\n",
      "Batch14\n",
      "loss 1.28883 valid_acc 0.505\n",
      "Batch15\n",
      "loss 1.27059 valid_acc 0.5045\n",
      "Batch16\n",
      "loss 1.176 valid_acc 0.5075\n",
      "Epoch10\n",
      "Batch1\n",
      "loss 1.24469 valid_acc 0.512\n",
      "Batch2\n",
      "loss 1.21204 valid_acc 0.5035\n",
      "Batch3\n",
      "loss 1.21372 valid_acc 0.4995\n",
      "Batch4\n",
      "loss 1.23979 valid_acc 0.5145\n",
      "Batch5\n",
      "loss 1.15271 valid_acc 0.51\n",
      "Batch6\n",
      "loss 1.22085 valid_acc 0.507\n",
      "Batch7\n",
      "loss 1.16728 valid_acc 0.5145\n",
      "Batch8\n",
      "loss 1.22316 valid_acc 0.5115\n",
      "Batch9\n",
      "loss 1.23652 valid_acc 0.514\n",
      "Batch10\n",
      "loss 1.31784 valid_acc 0.502\n",
      "Batch11\n",
      "loss 1.16972 valid_acc 0.506\n",
      "Batch12\n",
      "loss 1.22114 valid_acc 0.511\n",
      "Batch13\n",
      "loss 1.16678 valid_acc 0.5135\n",
      "Batch14\n",
      "loss 1.25441 valid_acc 0.5165\n",
      "Batch15\n",
      "loss 1.24191 valid_acc 0.511\n",
      "Batch16\n",
      "loss 1.13708 valid_acc 0.511\n",
      "Epoch11\n",
      "Batch1\n",
      "loss 1.21547 valid_acc 0.525\n",
      "Batch2\n",
      "loss 1.18036 valid_acc 0.5135\n",
      "Batch3\n",
      "loss 1.16898 valid_acc 0.5075\n",
      "Batch4\n",
      "loss 1.19709 valid_acc 0.5225\n",
      "Batch5\n",
      "loss 1.11513 valid_acc 0.5145\n",
      "Batch6\n",
      "loss 1.1998 valid_acc 0.518\n",
      "Batch7\n",
      "loss 1.14099 valid_acc 0.5175\n",
      "Batch8\n",
      "loss 1.19486 valid_acc 0.5215\n",
      "Batch9\n",
      "loss 1.20628 valid_acc 0.5205\n",
      "Batch10\n",
      "loss 1.26821 valid_acc 0.5115\n",
      "Batch11\n",
      "loss 1.13707 valid_acc 0.521\n",
      "Batch12\n",
      "loss 1.18726 valid_acc 0.5165\n",
      "Batch13\n",
      "loss 1.13288 valid_acc 0.5175\n",
      "Batch14\n",
      "loss 1.21114 valid_acc 0.5225\n",
      "Batch15\n",
      "loss 1.20425 valid_acc 0.5205\n",
      "Batch16\n",
      "loss 1.1016 valid_acc 0.521\n",
      "Epoch12\n",
      "Batch1\n",
      "loss 1.17489 valid_acc 0.529\n",
      "Batch2\n",
      "loss 1.14751 valid_acc 0.528\n",
      "Batch3\n",
      "loss 1.12932 valid_acc 0.525\n",
      "Batch4\n",
      "loss 1.1502 valid_acc 0.5265\n",
      "Batch5\n",
      "loss 1.07637 valid_acc 0.5325\n",
      "Batch6\n",
      "loss 1.16694 valid_acc 0.5325\n",
      "Batch7\n",
      "loss 1.1022 valid_acc 0.5225\n",
      "Batch8\n",
      "loss 1.16795 valid_acc 0.53\n",
      "Batch9\n",
      "loss 1.16688 valid_acc 0.5295\n",
      "Batch10\n",
      "loss 1.23227 valid_acc 0.5195\n",
      "Batch11\n",
      "loss 1.09512 valid_acc 0.528\n",
      "Batch12\n",
      "loss 1.15439 valid_acc 0.523\n",
      "Batch13\n",
      "loss 1.07997 valid_acc 0.535\n",
      "Batch14\n",
      "loss 1.17009 valid_acc 0.53\n",
      "Batch15\n",
      "loss 1.1694 valid_acc 0.5335\n",
      "Batch16\n",
      "loss 1.06049 valid_acc 0.527\n",
      "Epoch13\n",
      "Batch1\n",
      "loss 1.15731 valid_acc 0.538\n",
      "Batch2\n",
      "loss 1.10328 valid_acc 0.533\n",
      "Batch3\n",
      "loss 1.08007 valid_acc 0.526\n",
      "Batch4\n",
      "loss 1.12861 valid_acc 0.5315\n",
      "Batch5\n",
      "loss 1.03041 valid_acc 0.5315\n",
      "Batch6\n",
      "loss 1.12378 valid_acc 0.5425\n",
      "Batch7\n",
      "loss 1.05813 valid_acc 0.5285\n",
      "Batch8\n",
      "loss 1.12469 valid_acc 0.534\n",
      "Batch9\n",
      "loss 1.13551 valid_acc 0.528\n",
      "Batch10\n",
      "loss 1.18563 valid_acc 0.529\n",
      "Batch11\n",
      "loss 1.054 valid_acc 0.537\n",
      "Batch12\n",
      "loss 1.10557 valid_acc 0.542\n",
      "Batch13\n",
      "loss 1.05404 valid_acc 0.5365\n",
      "Batch14\n",
      "loss 1.14014 valid_acc 0.536\n",
      "Batch15\n",
      "loss 1.13121 valid_acc 0.542\n",
      "Batch16\n",
      "loss 1.0199 valid_acc 0.534\n",
      "Epoch14\n",
      "Batch1\n",
      "loss 1.10742 valid_acc 0.548\n",
      "Batch2\n",
      "loss 1.05745 valid_acc 0.538\n",
      "Batch3\n",
      "loss 1.04249 valid_acc 0.5375\n",
      "Batch4\n",
      "loss 1.07414 valid_acc 0.533\n",
      "Batch5\n",
      "loss 0.983279 valid_acc 0.5405\n",
      "Batch6\n",
      "loss 1.08002 valid_acc 0.5445\n",
      "Batch7\n",
      "loss 1.01645 valid_acc 0.534\n",
      "Batch8\n",
      "loss 1.10408 valid_acc 0.5365\n",
      "Batch9\n",
      "loss 1.0957 valid_acc 0.532\n",
      "Batch10\n",
      "loss 1.15212 valid_acc 0.536\n",
      "Batch11\n",
      "loss 1.01555 valid_acc 0.5405\n",
      "Batch12\n",
      "loss 1.07447 valid_acc 0.5405\n",
      "Batch13\n",
      "loss 1.02068 valid_acc 0.5405\n",
      "Batch14\n",
      "loss 1.09705 valid_acc 0.5435\n",
      "Batch15\n",
      "loss 1.09376 valid_acc 0.545\n",
      "Batch16\n",
      "loss 0.986548 valid_acc 0.547\n",
      "Epoch15\n",
      "Batch1\n",
      "loss 1.08186 valid_acc 0.5495\n",
      "Batch2\n",
      "loss 1.02849 valid_acc 0.5475\n",
      "Batch3\n",
      "loss 1.01432 valid_acc 0.5465\n",
      "Batch4\n",
      "loss 1.02336 valid_acc 0.5415\n",
      "Batch5\n",
      "loss 0.958701 valid_acc 0.548\n",
      "Batch6\n",
      "loss 1.05056 valid_acc 0.548\n",
      "Batch7\n",
      "loss 0.967446 valid_acc 0.5405\n",
      "Batch8\n",
      "loss 1.05203 valid_acc 0.544\n",
      "Batch9\n",
      "loss 1.07562 valid_acc 0.53\n",
      "Batch10\n",
      "loss 1.08274 valid_acc 0.545\n",
      "Batch11\n",
      "loss 0.956461 valid_acc 0.548\n",
      "Batch12\n",
      "loss 1.02004 valid_acc 0.5535\n",
      "Batch13\n",
      "loss 0.969157 valid_acc 0.5485\n",
      "Batch14\n",
      "loss 1.05995 valid_acc 0.5455\n",
      "Batch15\n",
      "loss 1.06025 valid_acc 0.5395\n",
      "Batch16\n",
      "loss 0.936681 valid_acc 0.55\n",
      "Epoch16\n",
      "Batch1\n",
      "loss 1.0436 valid_acc 0.5455\n",
      "Batch2\n",
      "loss 0.974909 valid_acc 0.5465\n",
      "Batch3\n",
      "loss 0.967418 valid_acc 0.5465\n",
      "Batch4\n",
      "loss 0.964088 valid_acc 0.5395\n",
      "Batch5\n",
      "loss 0.892559 valid_acc 0.538\n",
      "Batch6\n",
      "loss 0.997346 valid_acc 0.547\n",
      "Batch7\n",
      "loss 0.92752 valid_acc 0.538\n",
      "Batch8\n",
      "loss 1.02546 valid_acc 0.5455\n",
      "Batch9\n",
      "loss 1.03098 valid_acc 0.535\n",
      "Batch10\n",
      "loss 1.03658 valid_acc 0.5355\n",
      "Batch11\n",
      "loss 0.917136 valid_acc 0.5425\n",
      "Batch12\n",
      "loss 0.98865 valid_acc 0.554\n",
      "Batch13\n",
      "loss 0.926661 valid_acc 0.55\n",
      "Batch14\n",
      "loss 1.0141 valid_acc 0.5505\n",
      "Batch15\n",
      "loss 1.02174 valid_acc 0.5435\n",
      "Batch16\n",
      "loss 0.894815 valid_acc 0.5475\n",
      "Epoch17\n",
      "Batch1\n",
      "loss 0.9933 valid_acc 0.555\n",
      "Batch2\n",
      "loss 0.925419 valid_acc 0.5435\n",
      "Batch3\n",
      "loss 0.898715 valid_acc 0.555\n",
      "Batch4\n",
      "loss 0.918207 valid_acc 0.551\n",
      "Batch5\n",
      "loss 0.858182 valid_acc 0.5375\n",
      "Batch6\n",
      "loss 0.954034 valid_acc 0.553\n",
      "Batch7\n",
      "loss 0.847672 valid_acc 0.546\n",
      "Batch8\n",
      "loss 0.983814 valid_acc 0.554\n",
      "Batch9\n",
      "loss 0.980355 valid_acc 0.5415\n",
      "Batch10\n",
      "loss 0.992749 valid_acc 0.5445\n",
      "Batch11\n",
      "loss 0.88114 valid_acc 0.5565\n",
      "Batch12\n",
      "loss 0.934886 valid_acc 0.559\n",
      "Batch13\n",
      "loss 0.894047 valid_acc 0.5545\n",
      "Batch14\n",
      "loss 0.958688 valid_acc 0.544\n",
      "Batch15\n",
      "loss 0.983283 valid_acc 0.5425\n",
      "Batch16\n",
      "loss 0.869916 valid_acc 0.5475\n",
      "Epoch18\n",
      "Batch1\n",
      "loss 0.956104 valid_acc 0.5575\n",
      "Batch2\n",
      "loss 0.886817 valid_acc 0.547\n",
      "Batch3\n",
      "loss 0.860037 valid_acc 0.555\n",
      "Batch4\n",
      "loss 0.900466 valid_acc 0.5465\n",
      "Batch5\n",
      "loss 0.812469 valid_acc 0.539\n",
      "Batch6\n",
      "loss 0.903597 valid_acc 0.546\n",
      "Batch7\n",
      "loss 0.805504 valid_acc 0.5405\n",
      "Batch8\n",
      "loss 0.942916 valid_acc 0.5455\n",
      "Batch9\n",
      "loss 0.936887 valid_acc 0.5435\n",
      "Batch10\n",
      "loss 0.94488 valid_acc 0.5455\n",
      "Batch11\n",
      "loss 0.835726 valid_acc 0.5555\n",
      "Batch12\n",
      "loss 0.886597 valid_acc 0.564\n",
      "Batch13\n",
      "loss 0.849261 valid_acc 0.551\n",
      "Batch14\n",
      "loss 0.920543 valid_acc 0.5545\n",
      "Batch15\n",
      "loss 0.914737 valid_acc 0.5535\n",
      "Batch16\n",
      "loss 0.813531 valid_acc 0.562\n",
      "Epoch19\n",
      "Batch1\n",
      "loss 0.910886 valid_acc 0.552\n",
      "Batch2\n",
      "loss 0.820242 valid_acc 0.554\n",
      "Batch3\n",
      "loss 0.821815 valid_acc 0.546\n",
      "Batch4\n",
      "loss 0.834424 valid_acc 0.555\n",
      "Batch5\n",
      "loss 0.751234 valid_acc 0.5495\n",
      "Batch6\n",
      "loss 0.858495 valid_acc 0.5595\n",
      "Batch7\n",
      "loss 0.761169 valid_acc 0.5495\n",
      "Batch8\n",
      "loss 0.892867 valid_acc 0.5535\n",
      "Batch9\n",
      "loss 0.903365 valid_acc 0.5495\n",
      "Batch10\n",
      "loss 0.880816 valid_acc 0.5535\n",
      "Batch11\n",
      "loss 0.786576 valid_acc 0.5565\n",
      "Batch12\n",
      "loss 0.847963 valid_acc 0.564\n",
      "Batch13\n",
      "loss 0.802666 valid_acc 0.563\n",
      "Batch14\n",
      "loss 0.858173 valid_acc 0.5495\n",
      "Batch15\n",
      "loss 0.866454 valid_acc 0.5565\n",
      "Batch16\n",
      "loss 0.755199 valid_acc 0.5555\n",
      "Epoch20\n",
      "Batch1\n",
      "loss 0.85854 valid_acc 0.563\n",
      "Batch2\n",
      "loss 0.781879 valid_acc 0.558\n",
      "Batch3\n",
      "loss 0.762187 valid_acc 0.5635\n",
      "Batch4\n",
      "loss 0.774225 valid_acc 0.552\n",
      "Batch5\n",
      "loss 0.717913 valid_acc 0.548\n",
      "Batch6\n",
      "loss 0.804882 valid_acc 0.558\n",
      "Batch7\n",
      "loss 0.718167 valid_acc 0.5555\n",
      "Batch8\n",
      "loss 0.833228 valid_acc 0.5575\n",
      "Batch9\n",
      "loss 0.886204 valid_acc 0.555\n",
      "Batch10\n",
      "loss 0.840288 valid_acc 0.559\n",
      "Batch11\n",
      "loss 0.766914 valid_acc 0.5605\n",
      "Batch12\n",
      "loss 0.797576 valid_acc 0.5715\n",
      "Batch13\n",
      "loss 0.756593 valid_acc 0.565\n",
      "Batch14\n",
      "loss 0.805177 valid_acc 0.554\n",
      "Batch15\n",
      "loss 0.836348 valid_acc 0.5575\n",
      "Batch16\n",
      "loss 0.719286 valid_acc 0.5615\n"
     ]
    }
   ],
   "source": [
    "# Finally let's get training \n",
    "print(\"Training on the data\")\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(\"Epoch\"+ str(epoch+1))\n",
    "        num_batches = 16\n",
    "        for batch in range(num_batches):\n",
    "            print(\"Batch\"+ str(batch+1))\n",
    "            for offset in range(0, 2000, batch_size):\n",
    "                batch_x, batch_y = data_train[batch][offset:offset+batch_size], labels_train[batch][offset:offset+batch_size]\n",
    "                train_network(sess, optimizer, keep_probability, batch_x, batch_y )\n",
    "            print_stats(sess, batch_x, batch_y, cost, accuracy)     "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
