{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Importing the required packages\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>pixels</th>\n",
       "      <th>Usage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   emotion                                             pixels     Usage\n",
       "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n",
       "1        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training\n",
       "2        2  231 212 156 164 174 138 161 173 182 200 106 38...  Training\n",
       "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training\n",
       "4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('fer2013.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              pixels\n",
       "0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...\n",
       "1  151 150 147 155 148 133 111 140 170 174 182 15...\n",
       "2  231 212 156 164 174 138 161 173 182 200 106 38...\n",
       "3  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...\n",
       "4  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separating the data into labels and features\n",
    "labels, data = data['emotion'], data.drop(['emotion','Usage'], axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data = data[0:34000]\n",
    "labels = labels[0:34000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f1e9cb3a358>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nXvjGNIgXvjEN4oVvTIN44RvTIP8GhAeK/H0M050AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1e9ef2aef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Taking in the pixels of image i.e the features as a numpy array\n",
    "pixel_data = data.values\n",
    "pixel_data = pixel_data.astype(str)\n",
    "pixel_data = np.core.defchararray.rsplit(pixel_data, sep=None, maxsplit=None)\n",
    "\n",
    "# Visualizing the images using matplotlib\n",
    "first_row = np.asarray((pixel_data)[0][0]).astype('float32')\n",
    "vis_image = first_row.reshape(48,48)\n",
    "plt.imshow(vis_image,cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Reshaping Data\n",
    "\n",
    "This entire part focuses on reshaping the data to be fed into the placeholder tensors which are defined further in the code. This was probably one of the hardest parts of the code and had to be done after reading an extensive amount of numpy documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17, 2000, 48, 48, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining an empty list to hold values\n",
    "image_data = []\n",
    "# Here we parse through each element in pixel_data, extract and reshape the images and then finally resize it appropriately\n",
    "for i in range(len(pixel_data)):\n",
    "    row_data = np.asarray((pixel_data)[i][0]).astype('float32')\n",
    "    # Normalizing the data to a value between 0 and 1\n",
    "    row_data = row_data/255.0\n",
    "    reshaped_row_data = row_data.reshape(48,48)\n",
    "    image_data.append(reshaped_row_data)\n",
    "big_data = np.vstack(image_data)\n",
    "\n",
    "'''\n",
    "The shape of the array holding image data should be of size (number of batches, number of image samples, height, width, \n",
    "number of input color channels)\n",
    "'''\n",
    "final_data = big_data.reshape(34000,48,48,1)\n",
    "final_data = big_data.reshape(17,2000,48,48,1)\n",
    "final_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17, 2000, 7)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's set aside the image data and process the labels. Since labels are categorical variables we can one-hot-encode them  \n",
    "encoded_labels = pd.get_dummies(labels)\n",
    "encoded_labels = encoded_labels.as_matrix()\n",
    "encoded_labels = encoded_labels.reshape(17,2000,7)\n",
    "encoded_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Now to split the data into training and validation split\n",
    "from sklearn.cross_validation import train_test_split\n",
    "'''\n",
    "data_train, labels_train : Data and labels for training model \n",
    "data_test, labels_test : Data and labels for cross_validation \n",
    "'''\n",
    "data_train, data_test, labels_train, labels_test = train_test_split(final_data, encoded_labels, test_size=0.01, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_test = data_test.reshape(2000, 48, 48, 1)\n",
    "labels_test = labels_test.reshape(2000, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def neural_net_image_input(image_shape):\n",
    "    # Returns a tensor for holding input images\n",
    "    return tf.placeholder(tf.float32,shape=[None,image_shape[0],image_shape[1],image_shape[2]], name='x')\n",
    "         \n",
    "def neural_net_label_input(n_classes):\n",
    "    # Returns a tensor for holding image labels  \n",
    "    return tf.placeholder(tf.float32,shape=[None, n_classes], name='y')\n",
    "     \n",
    "def neural_net_keep_prob_input():\n",
    "    # Return a Tensor for keep probability\n",
    "    return tf.placeholder(tf.float32, shape=None, name='keep_prob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    depth = int(x_tensor.get_shape().as_list()[3])\n",
    "    weights = tf.Variable(tf.truncated_normal([conv_ksize[0],conv_ksize[1],depth,conv_num_outputs], stddev=0.05))\n",
    "    bias = tf.Variable(tf.zeros([conv_num_outputs]))\n",
    "    \n",
    "    # Convolutional layer + Maxpooling layer\n",
    "    conv_layer = tf.nn.conv2d(x_tensor, weights, strides=[1,conv_strides[0],conv_strides[1], 1], padding='SAME')\n",
    "    conv_layer = tf.nn.bias_add(conv_layer, bias)\n",
    "    conv_layer = tf.nn.relu(conv_layer)\n",
    "    conv_layer = tf.nn.max_pool(conv_layer, ksize=[1, pool_ksize[0], pool_ksize[1],1], strides=[1, pool_strides[0], pool_strides[1], 1], padding=\"SAME\")\n",
    "    return conv_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # Storing the dimensions of the tensor as a list into a variable to make it easily accessible.\n",
    "    list_dim = x_tensor.get_shape().as_list()\n",
    "    # Here the reshape function returns a tensor of reduced dimensionality, 2-D in this case with batch size as none.\n",
    "    flattened_tensor = tf.reshape(x_tensor, [-1, list_dim[1]*list_dim[2]*list_dim[3]])\n",
    "    return flattened_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    dimension = x_tensor.get_shape().as_list()[1]\n",
    "    # Here I'm defining separate weights and bias for the fully connected layer..these weights live inside of this function.\n",
    "    weights_full = tf.Variable(tf.truncated_normal([dimension, num_outputs], stddev = 0.05))\n",
    "    bias_full = tf.Variable(tf.zeros([num_outputs]))\n",
    "    # This is a regular hidden layer with weights and biases.\n",
    "    fully_connected = tf.add(tf.matmul(x_tensor, weights_full),bias_full)\n",
    "    fully_connected = tf.nn.relu(fully_connected)\n",
    "    return fully_connected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    new_dimension = x_tensor.get_shape().as_list()[1]\n",
    "    # This is very similar to the fully connected layer.\n",
    "    weights_out = tf.Variable(tf.truncated_normal([new_dimension, num_outputs], stddev = 0.05))\n",
    "    bias_out = tf.Variable(tf.zeros([num_outputs]))\n",
    "    #Implementing the output layer.\n",
    "    output_layer = tf.add(tf.matmul(x_tensor, weights_out), bias_out)\n",
    "    return output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\" \n",
    "    # The first three convolutional + maxpooling layers\n",
    "    neural_net = conv2d_maxpool(x, 32, (5,5), (1,1), (2,2), (1,1))\n",
    "    neural_net = conv2d_maxpool(neural_net, 64, (1,1), (2,2), (4,4), (2,2))\n",
    "    neural_net = conv2d_maxpool(neural_net, 64, (2,2), (1,1), (2,2), (1,1))\n",
    "    neural_net = conv2d_maxpool(neural_net, 128, (2,2), (1,1), (2,2), (1,1))\n",
    "\n",
    "    # Layer to flatten the tensor from the convolutional layers\n",
    "    neural_net = flatten(neural_net)\n",
    "    \n",
    "    # Two fully connected layers with dropout\n",
    "    neural_net = fully_conn(neural_net, 2304)\n",
    "    neural_net = tf.nn.dropout(neural_net, keep_prob)\n",
    "    neural_net = fully_conn(neural_net, 1152)\n",
    "    neural_net = tf.nn.dropout(neural_net, keep_prob)\n",
    "    neural_net = fully_conn(neural_net, 576)\n",
    "    \n",
    "    # Layer for output\n",
    "    neural_net = output(neural_net, 7)\n",
    "    \n",
    "    # TODO: return output\n",
    "    return neural_net\n",
    "\n",
    "##########Building the Neural Network##########\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((48, 48, 1))\n",
    "y = neural_net_label_input(7)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Two functions to train the network and print stats \n",
    "def train_network(session, optimizer, keep_probability, features,labels):\n",
    "    \n",
    "    # Optimizing the neural net\n",
    "    session.run(optimizer, feed_dict={x:features,y:labels, keep_prob:keep_probability})\n",
    "    pass\n",
    "\n",
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \n",
    "    # Calculating loss and validation accuracy\n",
    "    loss = sess.run(cost, feed_dict={x: feature_batch, y: label_batch, keep_prob: 1.})\n",
    "    valid_acc = sess.run(accuracy, feed_dict={\n",
    "                x: data_test,\n",
    "                y: labels_test,\n",
    "                keep_prob: 1.})\n",
    "    print(\"loss\", loss, \"valid_acc\", valid_acc)\n",
    "    pass    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Defining the hyperparameters\n",
    "epochs = 20\n",
    "batch_size = 200\n",
    "keep_probability = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on the data\n",
      "Epoch1\n",
      "Batch1\n",
      "loss 1.75724 valid_acc 0.2635\n",
      "Batch2\n",
      "loss 1.81617 valid_acc 0.2635\n",
      "Batch3\n",
      "loss 1.82886 valid_acc 0.2635\n",
      "Batch4\n",
      "loss 1.85866 valid_acc 0.2635\n",
      "Batch5\n",
      "loss 1.80269 valid_acc 0.2635\n",
      "Batch6\n",
      "loss 1.80255 valid_acc 0.2635\n",
      "Batch7\n",
      "loss 1.80167 valid_acc 0.2635\n",
      "Batch8\n",
      "loss 1.79886 valid_acc 0.2635\n",
      "Batch9\n",
      "loss 1.80933 valid_acc 0.2635\n",
      "Batch10\n",
      "loss 1.82077 valid_acc 0.2635\n",
      "Batch11\n",
      "loss 1.80484 valid_acc 0.2635\n",
      "Batch12\n",
      "loss 1.73304 valid_acc 0.266\n",
      "Batch13\n",
      "loss 1.77843 valid_acc 0.2665\n",
      "Batch14\n",
      "loss 1.78289 valid_acc 0.268\n",
      "Batch15\n",
      "loss 1.77112 valid_acc 0.284\n",
      "Batch16\n",
      "loss 1.79445 valid_acc 0.277\n",
      "Epoch2\n",
      "Batch1\n",
      "loss 1.7023 valid_acc 0.2735\n",
      "Batch2\n",
      "loss 1.76092 valid_acc 0.285\n",
      "Batch3\n",
      "loss 1.75479 valid_acc 0.2885\n",
      "Batch4\n",
      "loss 1.73217 valid_acc 0.2875\n",
      "Batch5\n",
      "loss 1.63777 valid_acc 0.287\n",
      "Batch6\n",
      "loss 1.71885 valid_acc 0.312\n",
      "Batch7\n",
      "loss 1.62907 valid_acc 0.318\n",
      "Batch8\n",
      "loss 1.63747 valid_acc 0.318\n",
      "Batch9\n",
      "loss 1.68456 valid_acc 0.3165\n",
      "Batch10\n",
      "loss 1.73049 valid_acc 0.326\n",
      "Batch11\n",
      "loss 1.60338 valid_acc 0.337\n",
      "Batch12\n",
      "loss 1.57396 valid_acc 0.3495\n",
      "Batch13\n",
      "loss 1.59341 valid_acc 0.361\n",
      "Batch14\n",
      "loss 1.6468 valid_acc 0.338\n",
      "Batch15\n",
      "loss 1.65982 valid_acc 0.3225\n",
      "Batch16\n",
      "loss 1.64828 valid_acc 0.3485\n",
      "Epoch3\n",
      "Batch1\n",
      "loss 1.52255 valid_acc 0.3785\n",
      "Batch2\n",
      "loss 1.56604 valid_acc 0.3615\n",
      "Batch3\n",
      "loss 1.61142 valid_acc 0.3695\n",
      "Batch4\n",
      "loss 1.55879 valid_acc 0.38\n",
      "Batch5\n",
      "loss 1.48193 valid_acc 0.3775\n",
      "Batch6\n",
      "loss 1.54646 valid_acc 0.3915\n",
      "Batch7\n",
      "loss 1.50368 valid_acc 0.3945\n",
      "Batch8\n",
      "loss 1.56614 valid_acc 0.393\n",
      "Batch9\n",
      "loss 1.57616 valid_acc 0.399\n",
      "Batch10\n",
      "loss 1.60618 valid_acc 0.4085\n",
      "Batch11\n",
      "loss 1.48465 valid_acc 0.4095\n",
      "Batch12\n",
      "loss 1.47815 valid_acc 0.413\n",
      "Batch13\n",
      "loss 1.49439 valid_acc 0.4055\n",
      "Batch14\n",
      "loss 1.56089 valid_acc 0.4155\n",
      "Batch15\n",
      "loss 1.53871 valid_acc 0.418\n",
      "Batch16\n",
      "loss 1.50101 valid_acc 0.412\n",
      "Epoch4\n",
      "Batch1\n",
      "loss 1.47368 valid_acc 0.4295\n",
      "Batch2\n",
      "loss 1.42817 valid_acc 0.4355\n",
      "Batch3\n",
      "loss 1.52588 valid_acc 0.4345\n",
      "Batch4\n",
      "loss 1.46614 valid_acc 0.4315\n",
      "Batch5\n",
      "loss 1.37875 valid_acc 0.428\n",
      "Batch6\n",
      "loss 1.44566 valid_acc 0.434\n",
      "Batch7\n",
      "loss 1.41513 valid_acc 0.4285\n",
      "Batch8\n",
      "loss 1.44727 valid_acc 0.436\n",
      "Batch9\n",
      "loss 1.44066 valid_acc 0.433\n",
      "Batch10\n",
      "loss 1.53255 valid_acc 0.444\n",
      "Batch11\n",
      "loss 1.42412 valid_acc 0.434\n",
      "Batch12\n",
      "loss 1.41169 valid_acc 0.4185\n",
      "Batch13\n",
      "loss 1.42954 valid_acc 0.447\n",
      "Batch14\n",
      "loss 1.48628 valid_acc 0.4475\n",
      "Batch15\n",
      "loss 1.43116 valid_acc 0.452\n",
      "Batch16\n",
      "loss 1.42798 valid_acc 0.4365\n",
      "Epoch5\n",
      "Batch1\n",
      "loss 1.41661 valid_acc 0.441\n",
      "Batch2\n",
      "loss 1.37982 valid_acc 0.4575\n",
      "Batch3\n",
      "loss 1.43596 valid_acc 0.4455\n",
      "Batch4\n",
      "loss 1.45132 valid_acc 0.4305\n",
      "Batch5\n",
      "loss 1.32626 valid_acc 0.433\n",
      "Batch6\n",
      "loss 1.37844 valid_acc 0.447\n",
      "Batch7\n",
      "loss 1.33625 valid_acc 0.4575\n",
      "Batch8\n",
      "loss 1.37664 valid_acc 0.4475\n",
      "Batch9\n",
      "loss 1.36694 valid_acc 0.448\n",
      "Batch10\n",
      "loss 1.50422 valid_acc 0.4615\n",
      "Batch11\n",
      "loss 1.37336 valid_acc 0.457\n",
      "Batch12\n",
      "loss 1.35373 valid_acc 0.469\n",
      "Batch13\n",
      "loss 1.35592 valid_acc 0.4645\n",
      "Batch14\n",
      "loss 1.41819 valid_acc 0.4595\n",
      "Batch15\n",
      "loss 1.38806 valid_acc 0.4455\n",
      "Batch16\n",
      "loss 1.36738 valid_acc 0.4585\n",
      "Epoch6\n",
      "Batch1\n",
      "loss 1.38666 valid_acc 0.461\n",
      "Batch2\n",
      "loss 1.29925 valid_acc 0.4635\n",
      "Batch3\n",
      "loss 1.35343 valid_acc 0.4585\n",
      "Batch4\n",
      "loss 1.35733 valid_acc 0.4515\n",
      "Batch5\n",
      "loss 1.27162 valid_acc 0.471\n",
      "Batch6\n",
      "loss 1.35987 valid_acc 0.4425\n",
      "Batch7\n",
      "loss 1.30119 valid_acc 0.4565\n",
      "Batch8\n",
      "loss 1.29791 valid_acc 0.4625\n",
      "Batch9\n",
      "loss 1.346 valid_acc 0.468\n",
      "Batch10\n",
      "loss 1.46261 valid_acc 0.46\n",
      "Batch11\n",
      "loss 1.31511 valid_acc 0.4725\n",
      "Batch12\n",
      "loss 1.3001 valid_acc 0.48\n",
      "Batch13\n",
      "loss 1.30495 valid_acc 0.471\n",
      "Batch14\n",
      "loss 1.37351 valid_acc 0.473\n",
      "Batch15\n",
      "loss 1.33385 valid_acc 0.47\n",
      "Batch16\n",
      "loss 1.3367 valid_acc 0.474\n",
      "Epoch7\n",
      "Batch1\n",
      "loss 1.35539 valid_acc 0.49\n",
      "Batch2\n",
      "loss 1.22496 valid_acc 0.496\n",
      "Batch3\n",
      "loss 1.31553 valid_acc 0.4805\n",
      "Batch4\n",
      "loss 1.31899 valid_acc 0.47\n",
      "Batch5\n",
      "loss 1.18473 valid_acc 0.472\n",
      "Batch6\n",
      "loss 1.29857 valid_acc 0.471\n",
      "Batch7\n",
      "loss 1.24486 valid_acc 0.462\n",
      "Batch8\n",
      "loss 1.26382 valid_acc 0.4785\n",
      "Batch9\n",
      "loss 1.29286 valid_acc 0.468\n",
      "Batch10\n",
      "loss 1.39325 valid_acc 0.4825\n",
      "Batch11\n",
      "loss 1.27991 valid_acc 0.483\n",
      "Batch12\n",
      "loss 1.24082 valid_acc 0.493\n",
      "Batch13\n",
      "loss 1.26803 valid_acc 0.48\n",
      "Batch14\n",
      "loss 1.32332 valid_acc 0.4785\n",
      "Batch15\n",
      "loss 1.27536 valid_acc 0.483\n",
      "Batch16\n",
      "loss 1.23678 valid_acc 0.4855\n",
      "Epoch8\n",
      "Batch1\n",
      "loss 1.33665 valid_acc 0.4965\n",
      "Batch2\n",
      "loss 1.22252 valid_acc 0.4915\n",
      "Batch3\n",
      "loss 1.24932 valid_acc 0.483\n",
      "Batch4\n",
      "loss 1.29241 valid_acc 0.479\n",
      "Batch5\n",
      "loss 1.14907 valid_acc 0.4755\n",
      "Batch6\n",
      "loss 1.27462 valid_acc 0.4865\n",
      "Batch7\n",
      "loss 1.22851 valid_acc 0.4695\n",
      "Batch8\n",
      "loss 1.23755 valid_acc 0.4755\n",
      "Batch9\n",
      "loss 1.24017 valid_acc 0.493\n",
      "Batch10\n",
      "loss 1.31569 valid_acc 0.5035\n",
      "Batch11\n",
      "loss 1.24923 valid_acc 0.484\n",
      "Batch12\n",
      "loss 1.20797 valid_acc 0.498\n",
      "Batch13\n",
      "loss 1.20317 valid_acc 0.4975\n",
      "Batch14\n",
      "loss 1.28631 valid_acc 0.489\n",
      "Batch15\n",
      "loss 1.2023 valid_acc 0.4925\n",
      "Batch16\n",
      "loss 1.18514 valid_acc 0.4945\n",
      "Epoch9\n",
      "Batch1\n",
      "loss 1.30334 valid_acc 0.5065\n",
      "Batch2\n",
      "loss 1.16798 valid_acc 0.4955\n",
      "Batch3\n",
      "loss 1.18274 valid_acc 0.496\n",
      "Batch4\n",
      "loss 1.16588 valid_acc 0.4975\n",
      "Batch5\n",
      "loss 1.1163 valid_acc 0.486\n",
      "Batch6\n",
      "loss 1.24696 valid_acc 0.4845\n",
      "Batch7\n",
      "loss 1.21154 valid_acc 0.4755\n",
      "Batch8\n",
      "loss 1.21453 valid_acc 0.488\n",
      "Batch9\n",
      "loss 1.18563 valid_acc 0.49\n",
      "Batch10\n",
      "loss 1.25674 valid_acc 0.5035\n",
      "Batch11\n",
      "loss 1.20722 valid_acc 0.4925\n",
      "Batch12\n",
      "loss 1.19455 valid_acc 0.504\n",
      "Batch13\n",
      "loss 1.16124 valid_acc 0.509\n",
      "Batch14\n",
      "loss 1.22174 valid_acc 0.4915\n",
      "Batch15\n",
      "loss 1.16398 valid_acc 0.4975\n",
      "Batch16\n",
      "loss 1.10972 valid_acc 0.503\n",
      "Epoch10\n",
      "Batch1\n",
      "loss 1.27712 valid_acc 0.5195\n",
      "Batch2\n",
      "loss 1.11275 valid_acc 0.5055\n",
      "Batch3\n",
      "loss 1.11448 valid_acc 0.5065\n",
      "Batch4\n",
      "loss 1.125 valid_acc 0.5005\n",
      "Batch5\n",
      "loss 1.01723 valid_acc 0.496\n",
      "Batch6\n",
      "loss 1.19122 valid_acc 0.495\n",
      "Batch7\n",
      "loss 1.12762 valid_acc 0.482\n",
      "Batch8\n",
      "loss 1.16537 valid_acc 0.505\n",
      "Batch9\n",
      "loss 1.1382 valid_acc 0.5055\n",
      "Batch10\n",
      "loss 1.13525 valid_acc 0.5015\n",
      "Batch11\n",
      "loss 1.15129 valid_acc 0.514\n",
      "Batch12\n",
      "loss 1.14431 valid_acc 0.5145\n",
      "Batch13\n",
      "loss 1.11223 valid_acc 0.512\n",
      "Batch14\n",
      "loss 1.19881 valid_acc 0.4955\n",
      "Batch15\n",
      "loss 1.13279 valid_acc 0.5115\n",
      "Batch16\n",
      "loss 1.08287 valid_acc 0.511\n",
      "Epoch11\n",
      "Batch1\n",
      "loss 1.21508 valid_acc 0.5125\n",
      "Batch2\n",
      "loss 1.10093 valid_acc 0.5135\n",
      "Batch3\n",
      "loss 1.05637 valid_acc 0.5085\n",
      "Batch4\n",
      "loss 1.09778 valid_acc 0.502\n",
      "Batch5\n",
      "loss 0.972489 valid_acc 0.5025\n",
      "Batch6\n",
      "loss 1.15826 valid_acc 0.5025\n",
      "Batch7\n",
      "loss 1.08788 valid_acc 0.49\n",
      "Batch8\n",
      "loss 1.15353 valid_acc 0.5\n",
      "Batch9\n",
      "loss 1.08579 valid_acc 0.512\n",
      "Batch10\n",
      "loss 1.07479 valid_acc 0.51\n",
      "Batch11\n",
      "loss 1.10321 valid_acc 0.518\n",
      "Batch12\n",
      "loss 1.11596 valid_acc 0.505\n",
      "Batch13\n",
      "loss 1.07857 valid_acc 0.5175\n",
      "Batch14\n",
      "loss 1.16918 valid_acc 0.499\n",
      "Batch15\n",
      "loss 1.08871 valid_acc 0.5055\n",
      "Batch16\n",
      "loss 1.00484 valid_acc 0.5185\n",
      "Epoch12\n",
      "Batch1\n",
      "loss 1.16209 valid_acc 0.5045\n",
      "Batch2\n",
      "loss 1.0671 valid_acc 0.522\n",
      "Batch3\n",
      "loss 1.03395 valid_acc 0.523\n",
      "Batch4\n",
      "loss 1.07137 valid_acc 0.5135\n",
      "Batch5\n",
      "loss 0.976995 valid_acc 0.4945\n",
      "Batch6\n",
      "loss 1.12756 valid_acc 0.5055\n",
      "Batch7\n",
      "loss 1.06422 valid_acc 0.5055\n",
      "Batch8\n",
      "loss 1.12513 valid_acc 0.516\n",
      "Batch9\n",
      "loss 1.05261 valid_acc 0.511\n",
      "Batch10\n",
      "loss 1.01123 valid_acc 0.512\n",
      "Batch11\n",
      "loss 1.08288 valid_acc 0.5165\n",
      "Batch12\n",
      "loss 1.03273 valid_acc 0.524\n",
      "Batch13\n",
      "loss 1.04767 valid_acc 0.5265\n",
      "Batch14\n",
      "loss 1.07687 valid_acc 0.5085\n",
      "Batch15\n",
      "loss 1.05757 valid_acc 0.5155\n",
      "Batch16\n",
      "loss 0.949768 valid_acc 0.5245\n",
      "Epoch13\n",
      "Batch1\n",
      "loss 1.05153 valid_acc 0.503\n",
      "Batch2\n",
      "loss 1.05022 valid_acc 0.514\n",
      "Batch3\n",
      "loss 1.03625 valid_acc 0.516\n",
      "Batch4\n",
      "loss 0.988505 valid_acc 0.502\n",
      "Batch5\n",
      "loss 0.932497 valid_acc 0.4975\n",
      "Batch6\n",
      "loss 1.08664 valid_acc 0.511\n",
      "Batch7\n",
      "loss 1.03312 valid_acc 0.488\n",
      "Batch8\n",
      "loss 1.1005 valid_acc 0.522\n",
      "Batch9\n",
      "loss 1.06861 valid_acc 0.5285\n",
      "Batch10\n",
      "loss 1.01859 valid_acc 0.4945\n",
      "Batch11\n",
      "loss 1.04531 valid_acc 0.535\n",
      "Batch12\n",
      "loss 1.0138 valid_acc 0.5195\n",
      "Batch13\n",
      "loss 0.995795 valid_acc 0.5265\n",
      "Batch14\n",
      "loss 1.05855 valid_acc 0.515\n",
      "Batch15\n",
      "loss 1.0242 valid_acc 0.5195\n",
      "Batch16\n",
      "loss 0.956914 valid_acc 0.524\n",
      "Epoch14\n",
      "Batch1\n",
      "loss 1.00714 valid_acc 0.495\n",
      "Batch2\n",
      "loss 1.01322 valid_acc 0.514\n",
      "Batch3\n",
      "loss 0.943978 valid_acc 0.524\n",
      "Batch4\n",
      "loss 0.953571 valid_acc 0.5235\n",
      "Batch5\n",
      "loss 0.896551 valid_acc 0.5045\n",
      "Batch6\n",
      "loss 1.03572 valid_acc 0.5095\n",
      "Batch7\n",
      "loss 0.960755 valid_acc 0.4995\n",
      "Batch8\n",
      "loss 1.06071 valid_acc 0.5235\n",
      "Batch9\n",
      "loss 1.02669 valid_acc 0.5245\n",
      "Batch10\n",
      "loss 0.881708 valid_acc 0.5035\n",
      "Batch11\n",
      "loss 1.00566 valid_acc 0.5195\n",
      "Batch12\n",
      "loss 0.960272 valid_acc 0.517\n",
      "Batch13\n",
      "loss 0.953691 valid_acc 0.5295\n",
      "Batch14\n",
      "loss 0.984971 valid_acc 0.5245\n",
      "Batch15\n",
      "loss 0.958338 valid_acc 0.531\n",
      "Batch16\n",
      "loss 0.899174 valid_acc 0.5245\n",
      "Epoch15\n",
      "Batch1\n",
      "loss 0.955058 valid_acc 0.484\n",
      "Batch2\n",
      "loss 0.954665 valid_acc 0.513\n",
      "Batch3\n",
      "loss 0.96174 valid_acc 0.533\n",
      "Batch4\n",
      "loss 0.914271 valid_acc 0.5345\n",
      "Batch5\n",
      "loss 0.865082 valid_acc 0.537\n",
      "Batch6\n",
      "loss 0.972595 valid_acc 0.5145\n",
      "Batch7\n",
      "loss 0.97423 valid_acc 0.516\n",
      "Batch8\n",
      "loss 1.10962 valid_acc 0.5155\n",
      "Batch9\n",
      "loss 1.02779 valid_acc 0.526\n",
      "Batch10\n",
      "loss 0.85472 valid_acc 0.518\n",
      "Batch11\n",
      "loss 0.962251 valid_acc 0.5165\n",
      "Batch12\n",
      "loss 0.990368 valid_acc 0.5245\n",
      "Batch13\n",
      "loss 0.909122 valid_acc 0.54\n",
      "Batch14\n",
      "loss 0.92445 valid_acc 0.52\n",
      "Batch15\n",
      "loss 0.960451 valid_acc 0.534\n",
      "Batch16\n",
      "loss 0.864236 valid_acc 0.514\n",
      "Epoch16\n",
      "Batch1\n",
      "loss 0.94652 valid_acc 0.494\n",
      "Batch2\n",
      "loss 0.96005 valid_acc 0.5145\n",
      "Batch3\n",
      "loss 0.945357 valid_acc 0.518\n",
      "Batch4\n",
      "loss 0.930965 valid_acc 0.536\n",
      "Batch5\n",
      "loss 0.879373 valid_acc 0.5285\n",
      "Batch6\n",
      "loss 0.984938 valid_acc 0.514\n",
      "Batch7\n",
      "loss 0.895669 valid_acc 0.5215\n",
      "Batch8\n",
      "loss 0.984641 valid_acc 0.531\n",
      "Batch9\n",
      "loss 0.997702 valid_acc 0.5235\n",
      "Batch10\n",
      "loss 0.812434 valid_acc 0.522\n",
      "Batch11\n",
      "loss 0.890238 valid_acc 0.5425\n",
      "Batch12\n",
      "loss 0.898571 valid_acc 0.5295\n",
      "Batch13\n",
      "loss 0.896838 valid_acc 0.534\n",
      "Batch14\n",
      "loss 0.874462 valid_acc 0.5105\n",
      "Batch15\n",
      "loss 0.885915 valid_acc 0.5375\n",
      "Batch16\n",
      "loss 0.827385 valid_acc 0.5065\n",
      "Epoch17\n",
      "Batch1\n",
      "loss 1.01274 valid_acc 0.521\n",
      "Batch2\n",
      "loss 0.955972 valid_acc 0.5225\n",
      "Batch3\n",
      "loss 0.901558 valid_acc 0.524\n",
      "Batch4\n",
      "loss 0.839063 valid_acc 0.5365\n",
      "Batch5\n",
      "loss 0.833475 valid_acc 0.5215\n",
      "Batch6\n",
      "loss 0.906988 valid_acc 0.522\n",
      "Batch7\n",
      "loss 0.827691 valid_acc 0.526\n",
      "Batch8\n",
      "loss 0.948699 valid_acc 0.535\n",
      "Batch9\n",
      "loss 0.944458 valid_acc 0.529\n",
      "Batch10\n",
      "loss 0.767301 valid_acc 0.5255\n",
      "Batch11\n",
      "loss 0.844411 valid_acc 0.537\n",
      "Batch12\n",
      "loss 0.871689 valid_acc 0.537\n",
      "Batch13\n",
      "loss 0.824691 valid_acc 0.5375\n",
      "Batch14\n",
      "loss 0.842767 valid_acc 0.5065\n",
      "Batch15\n",
      "loss 0.825076 valid_acc 0.5425\n",
      "Batch16\n",
      "loss 0.747528 valid_acc 0.524\n",
      "Epoch18\n",
      "Batch1\n",
      "loss 0.919387 valid_acc 0.5395\n",
      "Batch2\n",
      "loss 0.921434 valid_acc 0.5155\n",
      "Batch3\n",
      "loss 0.898263 valid_acc 0.5325\n",
      "Batch4\n",
      "loss 0.843394 valid_acc 0.5315\n",
      "Batch5\n",
      "loss 0.81378 valid_acc 0.5385\n",
      "Batch6\n",
      "loss 0.888231 valid_acc 0.529\n",
      "Batch7\n",
      "loss 0.801315 valid_acc 0.523\n",
      "Batch8\n",
      "loss 0.919903 valid_acc 0.5455\n",
      "Batch9\n",
      "loss 0.940126 valid_acc 0.526\n",
      "Batch10\n",
      "loss 0.775149 valid_acc 0.52\n",
      "Batch11\n",
      "loss 0.824234 valid_acc 0.5275\n",
      "Batch12\n",
      "loss 0.830091 valid_acc 0.5365\n",
      "Batch13\n",
      "loss 0.757459 valid_acc 0.5375\n",
      "Batch14\n",
      "loss 0.756992 valid_acc 0.5155\n",
      "Batch15\n",
      "loss 0.818308 valid_acc 0.5415\n",
      "Batch16\n",
      "loss 0.702249 valid_acc 0.542\n",
      "Epoch19\n",
      "Batch1\n",
      "loss 0.88545 valid_acc 0.5475\n",
      "Batch2\n",
      "loss 0.854928 valid_acc 0.529\n",
      "Batch3\n",
      "loss 0.796948 valid_acc 0.5345\n",
      "Batch4\n",
      "loss 0.762764 valid_acc 0.535\n",
      "Batch5\n",
      "loss 0.764374 valid_acc 0.5315\n",
      "Batch6\n",
      "loss 0.833492 valid_acc 0.533\n",
      "Batch7\n",
      "loss 0.682316 valid_acc 0.524\n",
      "Batch8\n",
      "loss 0.871494 valid_acc 0.519\n",
      "Batch9\n",
      "loss 0.888563 valid_acc 0.524\n",
      "Batch10\n",
      "loss 0.779515 valid_acc 0.5195\n",
      "Batch11\n",
      "loss 0.775359 valid_acc 0.5425\n",
      "Batch12\n",
      "loss 0.806333 valid_acc 0.529\n",
      "Batch13\n",
      "loss 0.730617 valid_acc 0.5325\n",
      "Batch14\n",
      "loss 0.675798 valid_acc 0.5155\n",
      "Batch15\n",
      "loss 0.793501 valid_acc 0.535\n",
      "Batch16\n",
      "loss 0.736161 valid_acc 0.531\n",
      "Epoch20\n",
      "Batch1\n",
      "loss 0.867038 valid_acc 0.516\n",
      "Batch2\n",
      "loss 0.836122 valid_acc 0.5065\n",
      "Batch3\n",
      "loss 0.849947 valid_acc 0.5215\n",
      "Batch4\n",
      "loss 0.789611 valid_acc 0.5385\n",
      "Batch5\n",
      "loss 0.684746 valid_acc 0.5335\n",
      "Batch6\n",
      "loss 0.818363 valid_acc 0.532\n",
      "Batch7\n",
      "loss 0.686039 valid_acc 0.533\n",
      "Batch8\n",
      "loss 0.841766 valid_acc 0.532\n",
      "Batch9\n",
      "loss 0.819133 valid_acc 0.529\n",
      "Batch10\n",
      "loss 0.726629 valid_acc 0.508\n",
      "Batch11\n",
      "loss 0.720779 valid_acc 0.5445\n",
      "Batch12\n",
      "loss 0.852183 valid_acc 0.533\n",
      "Batch13\n",
      "loss 0.738521 valid_acc 0.5265\n",
      "Batch14\n",
      "loss 0.668909 valid_acc 0.503\n",
      "Batch15\n",
      "loss 0.794319 valid_acc 0.533\n",
      "Batch16\n",
      "loss 0.678092 valid_acc 0.532\n"
     ]
    }
   ],
   "source": [
    "# Finally let's get training \n",
    "print(\"Training on the data\")\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(\"Epoch\"+ str(epoch+1))\n",
    "        num_batches = 16\n",
    "        for batch in range(num_batches):\n",
    "            print(\"Batch\"+ str(batch+1))\n",
    "            for offset in range(0, 2000, batch_size):\n",
    "                batch_x, batch_y = data_train[batch][offset:offset+batch_size], labels_train[batch][offset:offset+batch_size]\n",
    "                train_network(sess, optimizer, keep_probability, batch_x, batch_y )\n",
    "            print_stats(sess, batch_x, batch_y, cost, accuracy)     "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
